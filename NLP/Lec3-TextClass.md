# 文本分类

## Naive Bayesian

- $\bm{w}$ one-hot vector of vocab dictionary
- Target: $P(\bm{w} | \bm{c})$ $\bm{c}$ for cat

- Estimate likelihood: $\hat{P}(\bm w | \bm c) = \frac{n(\bm{w}_t)}{N_k}$ $n(w_t)$ number of class with $\bm{w}$ observed, $c_k$ total documents in $c_k$

## Text Rep

#### Latent Semantic Index

> Use context to represent a word
> LSA ??? 

