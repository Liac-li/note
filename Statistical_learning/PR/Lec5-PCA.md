# PCA

> 领域无关的特征提取
> PCA; Normalization; FLD

**减去平均值**:
    想使用一个 $y = w^T x + b$ 来使得目标维度 $y$ 减小，为了**简化优化过程**， 直接把 $x$ 中心化也即消除 $b$ 的影响 
    去除零阶表示 $b$ 

- 数据各个维度可能是不独立的，其内在维度 (intrinsic dimensionality) 远低于表面的维度

## 推导

#### 零阶表示

不允许使用任何一个维度，如何**最佳表示** $x$

- 寻找一个固定的 $\bm{m}$, 使得
    $$
        J_1(\bm{m}) = \min_m \sum_{i=1}^n ||\bm{x_i} - \bm{m}||^2
    $$

#### 一阶表示

- 如果投影方向上*变化*小，也即没什么效用，目标是投影方向变化大也即**方差**大

    $$
        J_2(w) = \frac{1}{n} \sum_{i=1}^n  ||w^T(x_i - \bar{x})||^2
    $$
    
    最大化方差, 但是 $J_2$ 中含有 $||w||^2$, 可以任意大小，要加上对于 $w$ 的限制

    $$
        \argmax_w \frac{1}{n} \sum_{i=1}^N ||(x_i - \bar{x})^T w||^2 \\
        s.t. \quad w^T w = 1
    $$
    
- assumption：数据服从高斯分布


## 在高斯分布上的 PCA


> PCA 不容易处理 outliner, 在 $Cov(x)$ 矩阵中影响较大